{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is one of the most widely used Python libraries in data science. We have imported the basic libraries that will help you perform the following commonly used data wrangling operations/tools in Pandas:\n",
    "\n",
    "* Creating DataFrames\n",
    "\n",
    "* Slicing DataFrames (i.e. selecting rows and columns)\n",
    "* Filtering data (using boolean arrays and groupby.filter)\n",
    "* Aggregating (using groupby.agg)\n",
    "* Visualizing data (using matplotlib.pyplot)\n",
    "\n",
    "We have provided 2 dummy CSV files (the `username` dataset and `email` dataset) to help you get started. Of course, feel free to import your own libraries and datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from js import fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the `fetch_and_read` function in order to read and store the data from the CSV files we are importing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_and_read(data_url):\n",
    "    res = await fetch(data_url)\n",
    "    text = await res.text()\n",
    "    \n",
    "    filename = 'data.csv'\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(text)\n",
    "        \n",
    "    data = pd.read_csv(filename, sep=';')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display the 5 rows of the `email` dataset for reference below. This dataset contains fake onboarding information of 5 new employees. The columns include:\n",
    "\n",
    "* `Login email` __(str):__ Employee login email\n",
    "\n",
    "* `Identifier` __(int):__ Unique employee ID\n",
    "\n",
    "* `First name` __(str):__ First name of employee\n",
    "\n",
    "* `Last name` __(str):__ Last name of employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = await fetch_and_read(\"https://support.staffbase.com/hc/en-us/article_attachments/360009197071/email.csv\")\n",
    "email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display the 5 rows of the `username` dataset for reference below. This dataset contains fake onboarding information of 5 new employees. The columns include:\n",
    "\n",
    "* `Username` __(str):__ Employee username\n",
    "\n",
    "* `Identifier` __(int):__ Unique employee ID\n",
    "\n",
    "* `One-time password` __(str):__ Employee access code\n",
    "\n",
    "* `Recovery code` __(str):__ Employee recovery code\n",
    "\n",
    "* `First name` __(str):__ First name of employee\n",
    "\n",
    "* `Last name` __(str):__ Last name of employee\n",
    "\n",
    "* `Department` __(str):__ Department that the employee belongs to\n",
    "\n",
    "* `Location` __(str):__ Where the employee is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = await fetch_and_read(\"https://support.staffbase.com/hc/en-us/article_attachments/360009197011/username-password-recovery-code.csv\")\n",
    "\n",
    "# Remove space in the column name \"Identifier\" in order to make merge easier later on by having uniform column names\n",
    "username = username.rename(columns={\" Identifier\": \"Identifier\"})\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Here, we are using `df.merge` to inner merge the `username` dataset with the `email` dataset to find what the corresponding emails are for each username. Notice how `booker12` didn't have an email and was thus left out of the merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_username_and_email = username.merge(email, how=\"inner\", on=\"Identifier\")\n",
    "employee_username_and_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Write your own code snippets here and create new cells as you see fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
